import select
import gradio as gr
import os
import shutil
from models.localdocumentqa import LocalDocumentQA
from config.config import *


embedding_model_dict_list = list(embedding_model_dict.keys())

llm_model_dict_list = list(llm_model_dict.keys())

local_document_qa = LocalDocumentQA()


def get_file_list():
    if not os.path.exists(FILE_CONTENT_PATH):
        return []
    return [f for f in os.listdir(FILE_CONTENT_PATH)]    

file_list = get_file_list()

def upload_file(file):
    if not os.path.exists(FILE_CONTENT_PATH):
        os.mkdir(FILE_CONTENT_PATH)

    filename = os.path.basename(file.name)
    shutil.move(file.name,FILE_CONTENT_PATH + filename)

    file_list.insert(0, filename)
    return gr.Dropdown.update(choices=file_list,value=filename)



def get_answer(query,vector_store_path,history):


    # if vector_store_path:
    #     resp, history = local_document_qa.get_knowledge_based_answer(query=query,vector_store_path=vector_store_path,chat_history=history)
    # else:
    #     history = history + [[None,"ËØ∑ÂÖàÂä†ËΩΩÊñá‰ª∂ÂêéÔºåÂÜçÊèêÈóÆ"]]

    for resp, history in local_document_qa.get_knowledge_based_answer(query=query, vector_store_path=vector_store_path, chat_history=history):
            source = "\n\n"
            source += "".join(
                [f"""<details> <summary>Âá∫Â§Ñ [{i + 1}] {os.path.split(doc.metadata["source"])[-1]}</summary>\n"""
                 f"""{doc.page_content}\n"""
                 f"""</details>"""
                 for i, doc in
                 enumerate(resp["source_documents"])])
            history[-1][-1] += source
            yield history, ""

    # return history,""


def update_status(history, status):
    history = history + [[None,status]]
    print(status)
    return history


def init_model():

    try:
        local_document_qa.init_cfg()
        # local_document_qa.llm._call("‰Ω†Â•Ω") 
        return """ Ê®°ÂûãÂ∑≤ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª"Âä†ËΩΩÊñá‰ª∂"ÊåâÈíÆ """
    except Exception as e:
        print(e)
        return """ Ê®°ÂûãÂä†ËΩΩÂºÇÂ∏∏ÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª"Âä†ËΩΩÊñá‰ª∂"ÊåâÈíÆ """

def reinit_model(llm_model,embedding_model,llm_history_len,top_k,history):

    try:
        local_document_qa.init_cfg(llm_model=llm_model,embedding_model=embedding_model,llm_history_len=llm_history_len,top_k=top_k)
        model_status = """ Ê®°ÂûãÂ∑≤ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª"Âä†ËΩΩÊñá‰ª∂"ÊåâÈíÆ """

    except:
        model_status = """ Ê®°ÂûãÂä†ËΩΩÂºÇÂ∏∏ÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª"Âä†ËΩΩÊñá‰ª∂"ÊåâÈíÆ """

    return history + [[None,model_status]]


def get_vector_store(filepath,history):

    if local_document_qa.llm and local_document_qa.llm:
        vector_store_path = local_document_qa.init_knowledge_vector_store([FILE_CONTENT_PATH + filepath])

        if vector_store_path:
            file_status = "Êñá‰ª∂Â∑≤ÁªèÊàêÂäüÂä†ËΩΩÔºåËØ∑ÂºÄÂßãÊèêÈóÆ"
        else:
            file_status = "Êñá‰ª∂Êú™Âä†ËΩΩÊàêÂäüÔºåËØ∑ÈáçÊñ∞‰∏ä‰º†"
    else:
        file_status = "Ê®°ÂûãÊú™Âä†ËΩΩÂÆåÊàêÔºåËØ∑ÂÖàÂä†ËΩΩÊ®°ÂûãÂêéÂÜçÂØºÂÖ•Êñá‰ª∂"
        vector_store_path = None

    return vector_store_path,history + [[None,file_status]]

block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}

.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""


webui_title = """
# üéâlangchain-chatglm-documentüéâ

üëç [https://github.com/sunny5156/langchain-chatglm-document](https://github.com/sunny5156/langchain-chatglm-document)


"""

init_message = """
Ê¨¢Ëøé‰ΩøÁî® langchain-chatglm-document Web UIÔºåÂºÄÂßãÊèêÈóÆÂâçÔºåËØ∑‰æùÊ¨°Â¶Ç‰∏ã 3 ‰∏™Ê≠•È™§Ôºö
1. ÈÄâÊã©ËØ≠Ë®ÄÊ®°Âûã„ÄÅEmbedding Ê®°ÂûãÂèäÁõ∏ÂÖ≥ÂèÇÊï∞ÂêéÁÇπÂáª"ÈáçÊñ∞Âä†ËΩΩÊ®°Âûã"ÔºåÂπ∂Á≠âÂæÖÂä†ËΩΩÂÆåÊàêÊèêÁ§∫
2. ‰∏ä‰º†ÊàñÈÄâÊã©Â∑≤ÊúâÊñá‰ª∂‰Ωú‰∏∫Êú¨Âú∞Áü•ËØÜÊñáÊ°£ËæìÂÖ•ÂêéÁÇπÂáª"ÈáçÊñ∞Âä†ËΩΩÊñáÊ°£"ÔºåÂπ∂Á≠âÂæÖÂä†ËΩΩÂÆåÊàêÊèêÁ§∫
3. ËæìÂÖ•Ë¶ÅÊèê‰∫§ÁöÑÈóÆÈ¢òÂêéÔºåÁÇπÂáªÂõûËΩ¶Êèê‰∫§"""

model_status = init_model()


if __name__ == "__main__":
    with gr.Blocks(css=block_css) as app:

        vector_store_path,file_status,model_status = gr.State(""),gr.State(""),gr.State(model_status)

        gr.Markdown(webui_title)

        with gr.Row():

            with gr.Column(scale=2):

                chatbot = gr.Chatbot([[None,init_message],[None,model_status.value]],elem_id="chat-box",show_label=False).style(height=750)

                query = gr.Textbox(show_label=False,placeholder="ËØ∑ËæìÂÖ•ÊèêÈóÆÂÜÖÂÆπÔºåÊåâÂõûËΩ¶ÈîÆÊèê‰∫§")


            with gr.Column(scale=1):
                llm_model = gr.Radio(llm_model_dict_list, label="LLM Ê®°Âûã",value=LLM_MODEL,interactive=True)

                llm_history_len = gr.Slider(0,10,value=3,step=1,label="LLM history len",interactive=True)

                embedding_model = gr.Radio(embedding_model_dict_list,label="Embedding Ê®°Âûã",value=EMBEDDING_MODEL,interactive=True)

                top_k = gr.Slider(1,20,value=6,step=1,label="ÂêëÈáèÂåπÈÖç topk",interactive=True)

                load_model_buttom = gr.Button("ÈáçÊñ∞Âä†ËΩΩÊ®°Âûã")

                with gr.Tab("select"):

                    selectFile = gr.Dropdown(file_list,label="content file",interactive=True,value=file_list[0] if len(file_list)>0 else None)

                with gr.Tab("upload"):
                    file = gr.File(label="content file",file_types=['.txt', '.md', '.docx', '.pdf'])
                
                load_file_button = gr.Button("Âä†ËΩΩÊñá‰ª∂")

        load_model_buttom.click(reinit_model,show_progress=True,inputs=[llm_model,embedding_model,llm_history_len,top_k,chatbot],outputs=chatbot)


        file.upload(upload_file,inputs=file,outputs=selectFile)

        load_file_button.click(get_vector_store,show_progress=True,inputs=[selectFile,chatbot],outputs=[vector_store_path,chatbot])

        query.submit(get_answer,inputs=[query,vector_store_path,chatbot],outputs=[chatbot,query])


    app.queue(concurrency_count=3).launch(server_name="0.0.0.0",share=False,inbrowser=False)
