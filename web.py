import select
import gradio as gr
import os
import shutil
from models.localdocumentqa import LocalDocumentQA
from config.config import *


embedding_model_dict_list = list(embedding_model_dict.keys())

llm_model_dict_list = list(llm_model_dict.keys())

local_document_qa = LocalDocumentQA()


def get_file_list():
    if not os.path.exists(FILE_CONTENT_PATH):
        return []
    return [f for f in os.listdir(FILE_CONTENT_PATH)]    

file_list = get_file_list()

def upload_file(file):
    if not os.path.exists(FILE_CONTENT_PATH):
        os.mkdir(FILE_CONTENT_PATH)

    filename = os.path.basename(file.name)
    shutil.move(file.name,FILE_CONTENT_PATH + filename)

    file_list.insert(0, filename)
    return gr.Dropdown.update(choices=file_list,value=filename)



def get_answer(query,vector_store_path,history):

    print("********************************\n",query,vector_store_path,history)

    if vector_store_path:
        resp, history = local_document_qa.get_knowledge_based_answer(query=query,vector_store_path=vector_store_path,chat_history=history)
    else:
        history = history + [[None,"è¯·å…ˆåŠ è½½æ–‡ä»¶åŽï¼Œå†æé—®"]]

    return history,""


def update_status(history, status):
    history = history + [[None,status]]
    print(status)
    return history


def init_model():

    try:
        local_document_qa.init_cfg()
        local_document_qa.llm._call("ä½ å¥½") 
        return """ æ¨¡åž‹å·²æˆåŠŸåŠ è½½ï¼Œè¯·é€‰æ‹©æ–‡ä»¶åŽç‚¹å‡»"åŠ è½½æ–‡ä»¶"æŒ‰é’® """
    except:
        return """ æ¨¡åž‹åŠ è½½å¼‚å¸¸ï¼Œè¯·é€‰æ‹©æ–‡ä»¶åŽç‚¹å‡»"åŠ è½½æ–‡ä»¶"æŒ‰é’® """

def reinit_model(llm_model,embedding_model,llm_history_len,top_k,history):

    try:
        local_document_qa.init_cfg(llm_model=llm_model,embedding_model=embedding_model,llm_history_len=llm_history_len,top_k=top_k)
        model_status = """ æ¨¡åž‹å·²æˆåŠŸåŠ è½½ï¼Œè¯·é€‰æ‹©æ–‡ä»¶åŽç‚¹å‡»"åŠ è½½æ–‡ä»¶"æŒ‰é’® """

    except:
        model_status = """ æ¨¡åž‹åŠ è½½å¼‚å¸¸ï¼Œè¯·é€‰æ‹©æ–‡ä»¶åŽç‚¹å‡»"åŠ è½½æ–‡ä»¶"æŒ‰é’® """

    return history + [[None,model_status]]


def get_vector_store(filepath,history):

    print("****************************************************************\n",FILE_CONTENT_PATH + filepath)

    if local_document_qa.llm and local_document_qa.llm:
        vector_store_path = local_document_qa.init_knowledge_vector_store(FILE_CONTENT_PATH + filepath)

        if vector_store_path:
            file_status = "æ–‡ä»¶å·²ç»æˆåŠŸåŠ è½½ï¼Œè¯·å¼€å§‹æé—®"
        else:
            file_status = "æ–‡ä»¶æœªåŠ è½½æˆåŠŸï¼Œè¯·é‡æ–°ä¸Šä¼ "
    else:
        file_status = "æ¨¡åž‹æœªåŠ è½½å®Œæˆï¼Œè¯·å…ˆåŠ è½½æ¨¡åž‹åŽå†å¯¼å…¥æ–‡ä»¶"
        vector_store_path = None

    return vector_store_path,history + [[None,file_status]]

block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}

.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""


webui_title = """
# ðŸŽ‰langchain-chatglm-documentðŸŽ‰

ðŸ‘ [https://github.com/sunny5156/langchain-chatglm-document](https://github.com/sunny5156/langchain-chatglm-document)


"""

init_message = """
æ¬¢è¿Žä½¿ç”¨ langchain-chatglm-document Web UIï¼Œå¼€å§‹æé—®å‰ï¼Œè¯·ä¾æ¬¡å¦‚ä¸‹ 3 ä¸ªæ­¥éª¤ï¼š
1. é€‰æ‹©è¯­è¨€æ¨¡åž‹ã€Embedding æ¨¡åž‹åŠç›¸å…³å‚æ•°åŽç‚¹å‡»"é‡æ–°åŠ è½½æ¨¡åž‹"ï¼Œå¹¶ç­‰å¾…åŠ è½½å®Œæˆæç¤º
2. ä¸Šä¼ æˆ–é€‰æ‹©å·²æœ‰æ–‡ä»¶ä½œä¸ºæœ¬åœ°çŸ¥è¯†æ–‡æ¡£è¾“å…¥åŽç‚¹å‡»"é‡æ–°åŠ è½½æ–‡æ¡£"ï¼Œå¹¶ç­‰å¾…åŠ è½½å®Œæˆæç¤º
3. è¾“å…¥è¦æäº¤çš„é—®é¢˜åŽï¼Œç‚¹å‡»å›žè½¦æäº¤"""

model_status = init_model()


if __name__ == "__main__":
    with gr.Blocks(css=block_css) as demo:

        vector_store_path,file_status,model_status = gr.State(""),gr.State(""),gr.State(model_status)

        gr.Markdown(webui_title)

        with gr.Row():

            with gr.Column(scale=2):

                chatbot = gr.Chatbot([[None,init_message],[None,model_status.value]],elem_id="chat-box",show_label=False).style(height=750)

                query = gr.Textbox(show_label=False,placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›žè½¦é”®æäº¤")


            with gr.Column(scale=1):
                llm_model = gr.Radio(llm_model_dict_list, label="LLM æ¨¡åž‹",value=LLM_MODEL,interactive=True)

                llm_history_len = gr.Slider(0,10,value=3,step=1,label="LLM history len",interactive=True)

                embedding_model = gr.Radio(embedding_model_dict_list,label="Embedding æ¨¡åž‹",value=EMBEDDING_MODEL,interactive=True)

                top_k = gr.Slider(1,20,value=6,step=1,label="å‘é‡åŒ¹é… topk",interactive=True)

                load_model_buttom = gr.Button("é‡æ–°åŠ è½½æ¨¡åž‹")

                with gr.Tab("select"):

                    selectFile = gr.Dropdown(file_list,label="content file",interactive=True,value=file_list[0] if len(file_list)>0 else None)

                with gr.Tab("upload"):
                    file = gr.File(label="content file",file_types=['.txt', '.md', '.docx', '.pdf'])
                
                load_file_button = gr.Button("åŠ è½½æ–‡ä»¶")

        load_model_buttom.click(reinit_model,show_progress=True,inputs=[llm_model,embedding_model,llm_history_len,top_k,chatbot],outputs=chatbot)


        file.upload(upload_file,inputs=file,outputs=selectFile)

        load_file_button.click(get_vector_store,show_progress=True,inputs=[selectFile,chatbot],outputs=[vector_store_path,chatbot])

        print("================================\n",query)

        query.submit(get_answer,inputs=[query,vector_store_path,chatbot],outputs=[chatbot,query])


    demo.queue(concurrency_count=3).launch(server_name="0.0.0.0",share=False,inbrowser=False)
